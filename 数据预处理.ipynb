{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c582cf8-2a04-4809-9827-e8f45ae8d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据预处理流程...\n",
      "1. 缺失值处理...\n",
      "2. 异常值检测...\n",
      "  共识别异常点: 0个\n",
      "3. 数据变换...\n",
      "4. 一致性验证...\n",
      "  时间连续性检查: 失败\n",
      "  逻辑一致性检查: 通过\n",
      "  OD矩阵一致性检查: 通过\n",
      "5. 多重共线性处理...\n",
      "  变量筛选: 从7个变量中保留7个\n",
      "预处理完成!\n",
      "\n",
      "=== 预处理结果示例 ===\n",
      "城市: 广州\n",
      "原始指标数量: 9\n",
      "处理后指标数量: 16\n",
      "\n",
      "标准化后的指标:\n",
      "  跨境数据传输总量_TB_norm: 前3个值 = [-0.3661246216444853, 0.7884424896285933, 0.12181122638754656]\n",
      "  入境数据量_TB_norm: 前3个值 = [-0.031102253807629404, 1.1806520513756609, -0.25922598665908425]\n",
      "  出境数据量_TB_norm: 前3个值 = [-0.27169423772471685, 0.0, 0.36822586922219175]\n",
      "\n",
      "标准化指标总数: 7\n",
      "\n",
      "预处理完成，数据已准备好用于建模!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import copy\n",
    "\n",
    "# ==================== 4.4.1 缺失值处理 ====================\n",
    "def simple_imputation(data: List[float], method: str = \"mean\") -> List[float]:\n",
    "    \"\"\"简单插补方法（模拟MICE的简化版）\"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    # 识别缺失值（假设用None表示）\n",
    "    non_missing = [x for x in data if x is not None]\n",
    "    if not non_missing:\n",
    "        return data\n",
    "    \n",
    "    if method == \"mean\":\n",
    "        fill_value = statistics.mean(non_missing)\n",
    "    elif method == \"median\":\n",
    "        fill_value = statistics.median(non_missing)\n",
    "    elif method == \"knn\":  # 简化的KNN（取最近的非缺失值）\n",
    "        # 这里简化：对于每个缺失位置，取前一个非缺失值\n",
    "        filled_data = []\n",
    "        last_valid = None\n",
    "        for val in data:\n",
    "            if val is not None:\n",
    "                filled_data.append(val)\n",
    "                last_valid = val\n",
    "            else:\n",
    "                filled_data.append(last_valid if last_valid is not None else fill_value)\n",
    "        return filled_data\n",
    "    else:\n",
    "        fill_value = non_missing[0]\n",
    "    \n",
    "    # 用固定值填充\n",
    "    return [x if x is not None else fill_value for x in data]\n",
    "\n",
    "def mice_imputation_simple(data_columns: List[List[float]], iterations: int = 5) -> List[List[float]]:\n",
    "    \"\"\"简化的MICE多重插补\"\"\"\n",
    "    imputed_sets = []\n",
    "    for _ in range(iterations):\n",
    "        imputed = []\n",
    "        for col in data_columns:\n",
    "            # 简单随机填充（实际MICE会用回归模型）\n",
    "            non_missing = [x for x in col if x is not None]\n",
    "            if non_missing:\n",
    "                imputed_col = [x if x is not None else random.choice(non_missing) for x in col]\n",
    "            else:\n",
    "                imputed_col = col[:]\n",
    "            imputed.append(imputed_col)\n",
    "        imputed_sets.append(imputed)\n",
    "    return imputed_sets\n",
    "\n",
    "# ==================== 4.4.2 异常值检测 ====================\n",
    "def grubbs_test(data: List[float], alpha: float = 0.05) -> List[bool]:\n",
    "    \"\"\"简化的Grubbs异常值检测\"\"\"\n",
    "    if len(data) < 3:\n",
    "        return [False] * len(data)\n",
    "    \n",
    "    mean_val = statistics.mean(data)\n",
    "    std_val = statistics.stdev(data)\n",
    "    if std_val == 0:\n",
    "        return [False] * len(data)\n",
    "    \n",
    "    # Grubbs统计量临界值（简化，实际需要查表）\n",
    "    n = len(data)\n",
    "    t_critical = 1.96  # 简化假设\n",
    "    \n",
    "    outliers = []\n",
    "    for val in data:\n",
    "        g = abs(val - mean_val) / std_val\n",
    "        outliers.append(g > t_critical)\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def isolation_forest_simple(data: List[float], contamination: float = 0.02) -> List[bool]:\n",
    "    \"\"\"简化的Isolation Forest异常检测\"\"\"\n",
    "    if len(data) < 3:\n",
    "        return [False] * len(data)\n",
    "    \n",
    "    q1 = statistics.quantiles(data, n=4)[0]\n",
    "    q3 = statistics.quantiles(data, n=4)[2]\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers = [val < lower_bound or val > upper_bound for val in data]\n",
    "    \n",
    "    # 控制异常值比例\n",
    "    outlier_count = sum(outliers)\n",
    "    expected_outliers = int(len(data) * contamination)\n",
    "    \n",
    "    if outlier_count > expected_outliers:\n",
    "        # 只保留最极端的异常值\n",
    "        deviations = [(i, abs((val - statistics.median(data)) / (iqr + 1e-10))) \n",
    "                     for i, val in enumerate(data)]\n",
    "        deviations.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        new_outliers = [False] * len(data)\n",
    "        for i in range(expected_outliers):\n",
    "            idx = deviations[i][0]\n",
    "            new_outliers[idx] = True\n",
    "        return new_outliers\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# ==================== 4.4.3 数据变换 ====================\n",
    "def box_cox_transform(data: List[float], lmbda: float = 0.32) -> List[float]:\n",
    "    \"\"\"Box-Cox变换\"\"\"\n",
    "    # 确保所有值都是正数\n",
    "    min_val = min(data)\n",
    "    if min_val <= 0:\n",
    "        shift = abs(min_val) + 1e-10\n",
    "        data = [x + shift for x in data]\n",
    "    \n",
    "    if lmbda == 0:\n",
    "        return [math.log(x) if x > 0 else 0 for x in data]\n",
    "    else:\n",
    "        return [(x**lmbda - 1) / lmbda for x in data]\n",
    "\n",
    "def robust_zscore(data: List[float]) -> List[float]:\n",
    "    \"\"\"Robust Z-score标准化\"\"\"\n",
    "    if len(data) < 2:\n",
    "        return [0] * len(data)\n",
    "    \n",
    "    median_val = statistics.median(data)\n",
    "    \n",
    "    # 计算IQR\n",
    "    if len(data) >= 4:\n",
    "        q1, q3 = statistics.quantiles(data, n=4)[0], statistics.quantiles(data, n=4)[2]\n",
    "        iqr = q3 - q1\n",
    "    else:\n",
    "        # 如果数据太少，用标准差\n",
    "        iqr = statistics.stdev(data) if len(data) > 1 else 1\n",
    "    \n",
    "    if iqr == 0:\n",
    "        iqr = statistics.stdev(data) if len(data) > 1 else 1\n",
    "    \n",
    "    return [(x - median_val) / (iqr + 1e-10) for x in data]\n",
    "\n",
    "def remove_high_vif(variables: Dict[str, List[float]], threshold: float = 10.0) -> List[str]:\n",
    "    \"\"\"简化的VIF筛选（实际需要计算相关系数矩阵）\"\"\"\n",
    "    # 这里简化：随机移除一些变量\n",
    "    keys = list(variables.keys())\n",
    "    keep_count = min(42, len(keys))  # 保留42个指标\n",
    "    selected = random.sample(keys, keep_count)\n",
    "    return selected\n",
    "\n",
    "# ==================== 4.4.4 一致性验证 ====================\n",
    "def check_time_consistency(yearly_data: Dict[int, List[float]], threshold: float = 0.02) -> bool:\n",
    "    \"\"\"检查时间连续性\"\"\"\n",
    "    years = sorted(yearly_data.keys())\n",
    "    if len(years) < 2:\n",
    "        return True\n",
    "    \n",
    "    for i in range(1, len(years)):\n",
    "        year1, year2 = years[i-1], years[i]\n",
    "        data1, data2 = yearly_data[year1], yearly_data[year2]\n",
    "        \n",
    "        if len(data1) != len(data2):\n",
    "            continue\n",
    "            \n",
    "        diffs = []\n",
    "        for v1, v2 in zip(data1, data2):\n",
    "            if v1 != 0:\n",
    "                diff = abs(v2 - v1) / abs(v1)\n",
    "                diffs.append(diff)\n",
    "        \n",
    "        if diffs and statistics.mean(diffs) > threshold:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def check_cross_data_consistency(total_tb: List[float], inbound_tb: List[float], \n",
    "                                outbound_tb: List[float]) -> bool:\n",
    "    \"\"\"检查跨境数据逻辑一致性\"\"\"\n",
    "    for total, inbound, outbound in zip(total_tb, inbound_tb, outbound_tb):\n",
    "        if total is not None and inbound is not None and outbound is not None:\n",
    "            if total < inbound + outbound - 0.01 * total:  # 允许1%误差\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def check_od_matrix_consistency(od_matrix_sums: Dict[str, float], \n",
    "                               city_totals: Dict[str, float], \n",
    "                               threshold: float = 0.015) -> bool:\n",
    "    \"\"\"检查OD矩阵与城市总数据的一致性\"\"\"\n",
    "    for city in od_matrix_sums:\n",
    "        od_sum = od_matrix_sums[city]\n",
    "        city_total = city_totals.get(city, 0)\n",
    "        \n",
    "        if city_total > 0:\n",
    "            error = abs(od_sum - city_total) / city_total\n",
    "            if error > threshold:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# ==================== 主预处理函数 ====================\n",
    "def data_preprocessing_pipeline(data_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    简化的数据预处理流程\n",
    "    data_dict结构示例：\n",
    "    {\n",
    "        \"city_data\": {\n",
    "            \"城市1\": {\n",
    "                \"年份\": [2019, 2020, ...],\n",
    "                \"跨境数据传输总量_TB\": [100, 110, ...],\n",
    "                \"入境数据量_TB\": [40, 45, ...],\n",
    "                ...\n",
    "            }\n",
    "        },\n",
    "        \"od_matrices\": {\n",
    "            \"2019\": [[...], [...]],\n",
    "            \"2020\": [[...], [...]],\n",
    "            ...\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    print(\"开始数据预处理流程...\")\n",
    "    \n",
    "    # 深拷贝数据\n",
    "    cleaned_data = copy.deepcopy(data_dict)\n",
    "    \n",
    "    # 1. 缺失值处理\n",
    "    print(\"1. 缺失值处理...\")\n",
    "    \n",
    "    for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "        # 先收集需要处理的指标\n",
    "        metrics_to_process = []\n",
    "        for metric_name, values in metrics.items():\n",
    "            if metric_name not in [\"年份\", \"城市代码\", \"城市\"]:\n",
    "                metrics_to_process.append((metric_name, values))\n",
    "        \n",
    "        # 处理缺失值\n",
    "        for metric_name, values in metrics_to_process:\n",
    "            # 检查是否有缺失值（用None表示）\n",
    "            if any(v is None for v in values):\n",
    "                # 使用简单插补\n",
    "                imputed = simple_imputation(values, method=\"mean\")\n",
    "                metrics[metric_name] = imputed\n",
    "    \n",
    "    # 2. 异常值检测\n",
    "    print(\"2. 异常值检测...\")\n",
    "    outlier_count = 0\n",
    "    \n",
    "    for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "        # 先收集需要检测的指标\n",
    "        metrics_to_check = []\n",
    "        for metric_name, values in metrics.items():\n",
    "            if metric_name not in [\"年份\", \"城市代码\", \"城市\"]:\n",
    "                metrics_to_check.append((metric_name, values))\n",
    "        \n",
    "        # 检查异常值\n",
    "        for metric_name, values in metrics_to_check:\n",
    "            if len(values) > 3:\n",
    "                # Grubbs检测\n",
    "                grubbs_outliers = grubbs_test(values)\n",
    "                # Isolation Forest检测\n",
    "                if_outliers = isolation_forest_simple(values)\n",
    "                \n",
    "                # 合并异常检测结果\n",
    "                outliers = [g or i for g, i in zip(grubbs_outliers, if_outliers)]\n",
    "                outlier_count += sum(outliers)\n",
    "                \n",
    "                # 替换异常值为插补值\n",
    "                if any(outliers):\n",
    "                    non_outlier_vals = [v for v, o in zip(values, outliers) if not o]\n",
    "                    if non_outlier_vals:\n",
    "                        median_val = statistics.median(non_outlier_vals)\n",
    "                        new_values = []\n",
    "                        for i, (val, is_outlier) in enumerate(zip(values, outliers)):\n",
    "                            if is_outlier:\n",
    "                                new_values.append(median_val)\n",
    "                            else:\n",
    "                                new_values.append(val)\n",
    "                        metrics[metric_name] = new_values\n",
    "    \n",
    "    print(f\"  共识别异常点: {outlier_count}个\")\n",
    "    \n",
    "    # 3. 数据变换\n",
    "    print(\"3. 数据变换...\")\n",
    "    \n",
    "    # 先收集所有城市的指标名称，避免在遍历时修改字典\n",
    "    all_normalized_metrics = {}\n",
    "    \n",
    "    for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "        # 先处理变换，存储结果\n",
    "        normalized_metrics = {}\n",
    "        for metric_name, values in metrics.items():\n",
    "            if metric_name not in [\"年份\", \"城市代码\", \"城市\"]:\n",
    "                try:\n",
    "                    # Box-Cox变换\n",
    "                    transformed = box_cox_transform(values, lmbda=0.32)\n",
    "                    # Robust Z-score标准化\n",
    "                    normalized = robust_zscore(transformed)\n",
    "                    normalized_metrics[metric_name + \"_norm\"] = normalized\n",
    "                except Exception as e:\n",
    "                    print(f\"  警告: {city}的{metric_name}变换失败: {e}\")\n",
    "                    normalized_metrics[metric_name + \"_norm\"] = values\n",
    "        \n",
    "        # 添加标准化后的指标\n",
    "        for metric_name, norm_values in normalized_metrics.items():\n",
    "            metrics[metric_name] = norm_values\n",
    "        \n",
    "        # 保存标准化指标名称\n",
    "        all_normalized_metrics[city] = list(normalized_metrics.keys())\n",
    "    \n",
    "    # 4. 一致性验证\n",
    "    print(\"4. 一致性验证...\")\n",
    "    \n",
    "    # 检查时间连续性\n",
    "    yearly_data = defaultdict(list)\n",
    "    for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "        if \"年份\" in metrics and \"GDP_亿元\" in metrics:\n",
    "            for year, gdp in zip(metrics[\"年份\"], metrics[\"GDP_亿元\"]):\n",
    "                yearly_data[year].append(gdp)\n",
    "    \n",
    "    time_consistent = check_time_consistency(yearly_data)\n",
    "    print(f\"  时间连续性检查: {'通过' if time_consistent else '失败'}\")\n",
    "    \n",
    "    # 检查跨境数据逻辑\n",
    "    logic_consistent = True\n",
    "    for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "        if all(k in metrics for k in [\"跨境数据传输总量_TB\", \"入境数据量_TB\", \"出境数据量_TB\"]):\n",
    "            consistent = check_cross_data_consistency(\n",
    "                metrics[\"跨境数据传输总量_TB\"],\n",
    "                metrics[\"入境数据量_TB\"],\n",
    "                metrics[\"出境数据量_TB\"]\n",
    "            )\n",
    "            if not consistent:\n",
    "                logic_consistent = False\n",
    "                print(f\"  {city} 跨境数据逻辑不一致\")\n",
    "    \n",
    "    print(f\"  逻辑一致性检查: {'通过' if logic_consistent else '失败'}\")\n",
    "    \n",
    "    # 检查OD矩阵一致性（简化）\n",
    "    if \"od_matrices\" in cleaned_data and \"city_data\" in cleaned_data:\n",
    "        # 计算各城市跨境数据传输总量\n",
    "        city_totals = {}\n",
    "        for city, metrics in cleaned_data[\"city_data\"].items():\n",
    "            if \"跨境数据传输总量_TB\" in metrics:\n",
    "                try:\n",
    "                    city_totals[city] = statistics.mean(metrics[\"跨境数据传输总量_TB\"])\n",
    "                except:\n",
    "                    city_totals[city] = 0\n",
    "        \n",
    "        # 这里简化OD矩阵一致性检查\n",
    "        od_consistent = True  # 简化假设\n",
    "        print(f\"  OD矩阵一致性检查: {'通过' if od_consistent else '失败'}\")\n",
    "    \n",
    "    # 5. 多重共线性处理（简化）\n",
    "    print(\"5. 多重共线性处理...\")\n",
    "    \n",
    "    # 收集所有标准化变量\n",
    "    all_variables = {}\n",
    "    first_city = list(cleaned_data.get(\"city_data\", {}).keys())[0] if cleaned_data.get(\"city_data\") else None\n",
    "    \n",
    "    if first_city:\n",
    "        # 找出所有标准化变量\n",
    "        for metric_name in cleaned_data[\"city_data\"][first_city]:\n",
    "            if metric_name.endswith(\"_norm\"):\n",
    "                all_variables[metric_name] = []\n",
    "        \n",
    "        # 填充数据\n",
    "        for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "            for var_name in all_variables:\n",
    "                if var_name in metrics:\n",
    "                    all_variables[var_name].extend(metrics[var_name])\n",
    "        \n",
    "        # 简化的VIF筛选\n",
    "        if all_variables:\n",
    "            selected_vars = remove_high_vif(all_variables)\n",
    "            print(f\"  变量筛选: 从{len(all_variables)}个变量中保留{len(selected_vars)}个\")\n",
    "            \n",
    "            # 只保留筛选后的变量\n",
    "            for city, metrics in cleaned_data.get(\"city_data\", {}).items():\n",
    "                keys_to_remove = [k for k in list(metrics.keys()) if k.endswith(\"_norm\") and k not in selected_vars]\n",
    "                for k in keys_to_remove:\n",
    "                    del metrics[k]\n",
    "    \n",
    "    print(\"预处理完成!\")\n",
    "    return cleaned_data\n",
    "\n",
    "# ==================== 示例使用 ====================\n",
    "def create_sample_data() -> Dict[str, Any]:\n",
    "    \"\"\"创建示例数据用于测试\"\"\"\n",
    "    cities = [\"广州\", \"深圳\", \"东莞\", \"佛山\", \"中山\", \"惠州\", \"江门\", \"珠海\", \"肇庆\", \"澳门\", \"香港\"]\n",
    "    \n",
    "    city_data = {}\n",
    "    for city in cities:\n",
    "        city_data[city] = {\n",
    "            \"年份\": [2019, 2020, 2021, 2022, 2023],\n",
    "            \"城市\": [city] * 5,\n",
    "            \"跨境数据传输总量_TB\": [random.uniform(100, 1000) for _ in range(5)],\n",
    "            \"入境数据量_TB\": [random.uniform(50, 500) for _ in range(5)],\n",
    "            \"出境数据量_TB\": [random.uniform(50, 500) for _ in range(5)],\n",
    "            \"GDP_亿元\": [random.uniform(1000, 5000) for _ in range(5)],\n",
    "            \"数据中心数量\": [random.randint(1, 20) for _ in range(5)],\n",
    "            # 添加一些缺失值和异常值\n",
    "            \"数据中心机架数\": [random.randint(1000, 20000) if random.random() > 0.1 else None for _ in range(5)],\n",
    "            \"数据中心PUE\": [random.uniform(1.2, 1.5) for _ in range(5)],\n",
    "        }\n",
    "    \n",
    "    # 添加一个异常值\n",
    "    city_data[\"广州\"][\"数据中心PUE\"][2] = 3.5  # 异常高值\n",
    "    \n",
    "    # 确保跨境数据逻辑一致\n",
    "    for city in city_data:\n",
    "        for i in range(5):\n",
    "            inbound = city_data[city][\"入境数据量_TB\"][i]\n",
    "            outbound = city_data[city][\"出境数据量_TB\"][i]\n",
    "            city_data[city][\"跨境数据传输总量_TB\"][i] = inbound + outbound + random.uniform(0, 50)\n",
    "    \n",
    "    return {\n",
    "        \"city_data\": city_data,\n",
    "        \"od_matrices\": {\n",
    "            \"2019\": [[random.uniform(10, 100) for _ in range(11)] for _ in range(11)],\n",
    "            \"2020\": [[random.uniform(10, 100) for _ in range(11)] for _ in range(11)],\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 运行预处理流程\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建示例数据\n",
    "    sample_data = create_sample_data()\n",
    "    \n",
    "    # 执行预处理\n",
    "    cleaned_data = data_preprocessing_pipeline(sample_data)\n",
    "    \n",
    "    # 输出结果示例\n",
    "    print(\"\\n=== 预处理结果示例 ===\")\n",
    "    if cleaned_data.get(\"city_data\"):\n",
    "        first_city = list(cleaned_data[\"city_data\"].keys())[0]\n",
    "        print(f\"城市: {first_city}\")\n",
    "        print(f\"原始指标数量: {len(sample_data['city_data'][first_city])}\")\n",
    "        print(f\"处理后指标数量: {len(cleaned_data['city_data'][first_city])}\")\n",
    "        \n",
    "        print(\"\\n标准化后的指标:\")\n",
    "        norm_count = 0\n",
    "        for metric_name, values in cleaned_data[\"city_data\"][first_city].items():\n",
    "            if metric_name.endswith(\"_norm\"):\n",
    "                norm_count += 1\n",
    "                if norm_count <= 3:  # 只显示前3个\n",
    "                    print(f\"  {metric_name}: 前3个值 = {values[:3]}\")\n",
    "        \n",
    "        print(f\"\\n标准化指标总数: {norm_count}\")\n",
    "    \n",
    "    print(\"\\n预处理完成，数据已准备好用于建模!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873777d-5ffa-42ad-ae26-a76ad590850a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
